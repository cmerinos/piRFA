% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/piRFA.R
\name{piRFA}
\alias{piRFA}
\title{Function to Analyze DIF with PI-RFA/MIMIC (Product of Indicators)}
\usage{
piRFA(
  data,
  items,
  cov,
  lvname = "LatFact",
  est = "MLM",
  Oort.adj = FALSE,
  p.crit = 0.05
)
}
\arguments{
\item{data}{DataFrame containing items and the covariate.}

\item{items}{Vector of item names within `data`.}

\item{cov}{Name of the covariate in `data` (can be categorical or numeric). If categorical, it must be a factor.}

\item{lvname}{Name for the latent variable in the model (default is `"LatFact"`).}

\item{est}{Abbreviation of the estimator to use. Must be a non-empty string (see lavaan documentation).}

\item{Oort.adj}{Logical. If `TRUE`, applies Oort's critical value adjustment to likelihood ratio (LR) tests.
Default is `FALSE`.}

\item{p.crit}{Numeric. Significance level used to compute the chi-square critical value (e.g., 0.05, 0.01, 0.10).
Only used if `Oort.adj = TRUE`. Default is `0.05`.}
}
\value{
The function returns a list with the following DataFrames:
 \itemize{
   \item \code{DIF.Global} - Global DIF results (includes Oort-adjusted critical value if requested).
   \item \code{DIF.Uniforme} - Uniform DIF results.
   \item \code{DIF.NoUniforme} - Non-uniform DIF results.
   \item \code{SEPC.uDIF} - SEPC coefficients for uniform DIF.
   \item \code{SEPC.nuDIF} - SEPC coefficients for non-uniform DIF.
 }
}
\description{
The `piRFA` function analyzes Differential Item Functioning (DIF)
using the multiple-indicators multiple-causes (MIMIC) framework with product of indicators (PI).
It relies on `lavaan` and `scripty`. Uniform and non-uniform DIF can be evaluated in a measurement scale,
through statistical tests and an effect size approximation.
}
\details{
This function implements Differential Item Functioning (DIF) analysis using
the Product of Indicators approach (PI; Kolbe & Jorgensen, 2018) within the
Restricted Factor Analysis (RFA; Oort, 1998) framework. This method operates under a
MIMIC scheme (Finch, 2005), incorporating latent variable interactions using PI (Kolbe et al., 2018, 2019;
Kolbe, Jorgensen, & Molenaar, 2020). It allows for the evaluation of uniform (uDIF) and non-uniform DIF (nuDIF)
with covariates that can be categorical (e.g., sex) or continuous (e.g., self-esteem, conscientiousness).

Estimation is performed via `lavaan::cfa`, and DIF statistical tests are based
on the likelihood ratio test (LRT). By default, chi-square tests compare an unrestricted model
(with covariate effect and interaction parameters freely estimated) and a restricted model
(with these parameters fixed to zero), using the standard chi-square distribution.

However, simulation studies (Oort, 1992, 1998; Kim, Yoon & Lee, 2011/2012) have shown that
the LR test under MIMIC may suffer from inflated Type I error rates. To address this, Oort proposed
a correction of the chi-square critical value:

\deqn{K' = \left( \frac{\chi^2_0}{K + df_0 - 1} \right) K}

where:
\itemize{
  \item \eqn{K'} is the adjusted critical value,
  \item \eqn{K} is the original critical value from the chi-square distribution at significance level \eqn{p.crit},
  \item \eqn{\chi^2_0} is the chi-square statistic of the baseline model,
  \item \eqn{df_0} is the corresponding degrees of freedom of the baseline model.
}

Interpretation:
\itemize{
  \item The reported \code{p.value} corresponds to the standard chi-square test.
  \item When \code{Oort.adj = TRUE}, the function also reports the adjusted critical value (\code{crit.Oort}).
  \item Users can compare the observed chi-square statistic against both thresholds:
        the conventional critical value (via \code{p.value}) and the Oort-adjusted critical value.
  \item This comparison allows assessment of how conclusions may differ when controlling for
        potential inflation of Type I error in MIMIC-PI models.
}

Note: The Oort adjustment can yield very large critical values when the baseline model has a high chi-square
relative to its degrees of freedom (e.g., in short tests, 3 or 4 items). This reflects the conservative
nature of the adjustment: Type I error is strongly controlled, but power to detect DIF may be
substantially reduced. Users are advised to report both the standard and the Oort-adjusted results,
and interpret them in light of overall model fit.
}
\examples{
### Example 1: simulated data -------------
set.seed(123)
Exmp1.data <- data.frame(
  grp = sample(0:1, 100, replace = TRUE),  # Group variable
  item1 = sample(1:5, 100, replace = TRUE),
  item2 = sample(1:5, 100, replace = TRUE),
  item3 = sample(1:5, 100, replace = TRUE),
  item4 = sample(1:5, 100, replace = TRUE))

res1 <- piRFA(data = Exmp1.data , items = c("item1","item2","item3"), cov = "grp")
res1$DIF.Global

### Example 2: Using the 'bfi' dataset from the 'psych' package -------------
library(psych)
data("bfi")

data.bfi <- bfi[, c("N1","N2","N3","N4","N5","gender")]
data.bfi <- data.bfi[complete.cases(data.bfi), ]
data.bfi$gender <- as.factor(data.bfi$gender)

neuro.items <- c("N1","N2","N3","N4","N5")

# Run DIF analysis with Oort adjustment
res.bfi <- piRFA(data = data.bfi, items = neuro.items, cov = "gender",
                 lvname = "Neuroticism", est = "MLM",
                 Oort.adj = TRUE, p.crit = 0.05)
res.bfi$DIF.Global

}
\references{
Kim, E. S., Yoon, M., & Lee, T. (2011). Testing Measurement Invariance Using MIMIC:
Likelihood Ratio Test With a Critical Value Adjustment. *Educational and Psychological Measurement, 72*(3), 469–492.
https://doi.org/10.1177/0013164411427395 (Original work published 2012)

Oort, F. J. (1992). Using restricted factor analysis to detect item bias. *Psychological Methods*, 37, 547–567.

Oort, F. J. (1998). Simulation study of item bias detection with restricted factor analysis.
*Structural Equation Modeling*, 5, 107–124.

French, B. F., & Finch, W. H. (2008). Multigroup confirmatory factor analysis: Locating the invariant referent variables.
*Structural Equation Modeling*, 15(1), 96–113.

Stark, S., Chernyshenko, O. S., & Drasgow, F. (2006). Detecting differential item functioning with confirmatory factor analysis and item response theory: Toward a unified strategy.
*Journal of Applied Psychology*, 91(6), 1292–1306.

Kolbe, L., & Jorgensen, T. D. (2018). Using product indicators in restricted factor analysis models to detect nonuniform measurement bias.
In *Quantitative Psychology* (pp. 235–245). Springer.

Kolbe, L., & Jorgensen, T. D. (2019). Using restricted factor analysis to select anchor items and detect differential item functioning.
*Behavior Research Methods*, 51, 138–151.

Kolbe, L., Jorgensen, T. D., & Molenaar, D. (2020). The Impact of Unmodeled Heteroskedasticity on Assessing Measurement Invariance in Single-group Models.
*Structural Equation Modeling*, 28(1), 82–98.

Whittaker, T. A. (2012). Estimation of Standardized Expected Parameter Change for DIF Detection.
*Educational and Psychological Measurement*, 72(3), 342-357.

Garnier-Villarreal, M., & Jorgensen, T. D. (2024). Evaluating Local Model Misspecification with Modification Indices in Bayesian Structural Equation Modeling.
*Structural Equation Modeling*, 1–15.
}
